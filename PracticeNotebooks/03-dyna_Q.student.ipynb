{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6646aa11",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the\n",
    " LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0e952",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook we study model-based reinforcement learning algorithms.\n",
    "We investigate two ways of learning a model of the transition function,\n",
    "using either a stochastic or a deterministic model.\n",
    "\n",
    "Besides, we compare the sample efficiency of learning a model of the transition function using random actions then learning the Q-function without new samples\n",
    "versus using the Dyna-Q algorithm, which simultaneously learns a model and uses it \"in imagination\" to improve the policy of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b64e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install 'easypip>=1.2.0'\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"gymnasium\")\n",
    "easyinstall(\"mazemdp\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f92adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "easyimport(\"bbrl_gymnasium\")\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tqdm was removed from q-learning and sarsa because when we use them\n",
    "# many times in a loop (e.g. for hyper-param tuning), it slows down the computation a lot\n",
    "if is_notebook() and get_ipython().__class__.__module__ != \"google.colab._shell\":\n",
    "   from tqdm.autonotebook import tqdm\n",
    "else:\n",
    "   from tqdm.auto import tqdm\n",
    "\n",
    "from mazemdp.toolbox import egreedy, softmax, sample_categorical\n",
    "from mazemdp.mdp import Mdp\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "# For visualization\n",
    "os.environ[\"VIDEO_FPS\"] = \"5\"\n",
    "if not os.path.isdir(\"./videos\"):\n",
    "    os.mkdir(\"./videos\")\n",
    "\n",
    "from IPython.display import Video\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09376ca2",
   "metadata": {},
   "source": [
    "# Model-Based Reinforcement Learning\n",
    "\n",
    "Model-Based Reinforcement Learning (MBRL) is an approach to RL where the agent learns a model of the transition function and uses it to update its value function and/or its policy. To learn the model of the transition function, the agent needs to interact with the environment. It can do so either with a fixed exploration policy or with the policy it is learning simultaneously with model of the transition function. To improve its target policy, there are several MBRL approaches:\n",
    "\n",
    "- the agent can apply dynamic programming (value iteration or policy iteration) algorithms using the learned model of the transition function\n",
    "\n",
    "- the agent can draw random transitions from the learned model and perform Bellman backups using these samples to update a critic model. This is called learning in imagination, and this corresponds to Dyna-Q when the agent uses the Q-learning update rule to perform Bellman backups.\n",
    "\n",
    "- the agent can do the same as above, but drawing transitions efficiently instead of randomly. This corresponds to Prioritized sweeping and its variants.\n",
    "\n",
    "As for model-free reinforcement learning, we first create a maze-like MDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edceec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import bbrl_gymnasium\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "\n",
    "# Environment with 20% of walls and no negative reward when hitting a wall\n",
    "env = gym.make(\"MazeMDP-v0\", kwargs={\"width\": 4, \"height\": 3, \"ratio\": 0.2, \"hit\": 0.0}, render_mode=\"rgb_array\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ca2df",
   "metadata": {},
   "source": [
    "## Transition models\n",
    "\n",
    "We create two TransitionModel classes that contain the learned model of the transition function.\n",
    "Both models inherit from a more generic TransitionModel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871cd27",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TransitionModel():\n",
    "   def __init__(self, env):\n",
    "      self.nb_states = env.unwrapped.nb_states # the .unwrapped removes a warning from gymnasium\n",
    "      self.nb_actions = env.action_space.n\n",
    "\n",
    "   def predict(self, state, action):\n",
    "      pass\n",
    "\n",
    "   def add_transition(self, state, action, next_state):\n",
    "      pass\n",
    "      \n",
    "   # This function draws a random transition from the model.\n",
    "\n",
    "   def sample_transition(self): # in BBRL, this will be renamed into forward\n",
    "      state = random.randint(0, self.nb_states - 1)\n",
    "      action = random.randint(0, self.nb_actions - 1)\n",
    "      next_state = self.predict(state, action)\n",
    "      return state, action, next_state\n",
    "\n",
    "   # To monitor the accuracy of the model of the transition function, we build an evaluate function.\n",
    "   # This function draws sample_size random transitions from the real maze and checks whether the model outputs the same next state.\n",
    "   # This way to proceed is only adequate if the real maze is deterministic.\n",
    "   # Otherwise, we should tell the distance between two probability distributions.\n",
    "\n",
    "   def evaluate_random(self, env, sample_size: int=100) -> int:\n",
    "      success = 0\n",
    "      for _ in range(sample_size):\n",
    "         state, action, next_state = env.sample_transition()\n",
    "         next_state_model = self.predict(state, action)\n",
    "         if next_state == next_state_model:\n",
    "            success = success + 1\n",
    "      return success / 100\n",
    "\n",
    "   def is_accurate(self, env) -> bool:\n",
    "      pass\n",
    "\n",
    "   # Useful for debug\n",
    "   \n",
    "   def display_count(self):\n",
    "      print(\"count :\", self.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284ebd2",
   "metadata": {},
   "source": [
    "The first model is stochastic, it is initialized with uniform probabilities and updates the probabilities when receiving new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fa751",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class StochasticTransitionModel(TransitionModel):\n",
    "   def __init__(self, env):\n",
    "      super().__init__(env)\n",
    "      self.M = np.ones(\n",
    "            (self.nb_states, self.nb_actions, self.nb_states)\n",
    "         ) / self.nb_states\n",
    "      self.I = np.array(range(self.nb_states))\n",
    "      self.count = np.zeros((self.nb_states, self.nb_actions))\n",
    "\n",
    "   def predict(self, state, action):\n",
    "      next_state = sample_categorical(self.M[state, action, :])\n",
    "      return next_state\n",
    "\n",
    "   def add_transition(self, state, action, next_state):\n",
    "      self.count[state, action] = self.count[state, action] + 1\n",
    "      self.M[state, action, :] = (1-1/self.count[state, action]) * (self.M[state, action, :]).reshape(self.nb_states) + 1/self.count[state, action] * np.transpose((self.I==next_state).astype(int))\n",
    "\n",
    "   # In the probabilistic case, sampling the right next state is not enough, as it may happen by chance\n",
    "   # we compare the probabilities\n",
    "\n",
    "   def is_accurate(self, env) -> bool:\n",
    "      for state in range(self.nb_states):\n",
    "         for action in range(self.nb_actions):\n",
    "            ground_truth_proba = env.unwrapped.P[state, action, :]\n",
    "            model_proba = self.M[state, action, :]\n",
    "            error = np.linalg.norm(ground_truth_proba - model_proba)\n",
    "            if error > 0.01:\n",
    "               # print(ground_truth_proba, model_proba, error)\n",
    "               return False\n",
    "      return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666df03",
   "metadata": {},
   "source": [
    "The second model is determistic, it is initialized as empty and learns new deterministic transitions as they come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827e541",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DeterministicTransitionModel(TransitionModel):\n",
    "   def __init__(self, env):\n",
    "      super().__init__(env)\n",
    "      self.count = np.zeros(\n",
    "            (self.nb_states, self.nb_actions, self.nb_states)\n",
    "         )\n",
    "\n",
    "   def predict(self, state, action):\n",
    "      next_state = np.argmax(self.count[state, action, :])\n",
    "      return next_state\n",
    "\n",
    "   def add_transition(self, state, action, next_state):\n",
    "      self.count[state, action, next_state] = self.count[state, action, next_state] + 1\n",
    "\n",
    "   # This function draws all transitions from the real maze and checks whether the model outputs the same next state\n",
    "   # This way to proceed is only adequate if the real maze is deterministic.\n",
    "\n",
    "   def is_accurate(self, env) -> bool:\n",
    "      # To be completed...\n",
    "\n",
    "      assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53e5cb",
   "metadata": {},
   "source": [
    "## Reward model\n",
    "\n",
    "This class is used to learn a model of the reward function.\n",
    "We consider that the reward function is deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d709a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RewardModel():\n",
    "   def __init__(self, env):\n",
    "      self.nb_states = env.unwrapped.nb_states\n",
    "      self.nb_actions = env.action_space.n\n",
    "      self.reward_model = np.zeros((self.nb_states, self.nb_actions))\n",
    "\n",
    "   def predict(self, state, action):\n",
    "      return self.reward_model[state, action]\n",
    "\n",
    "   def learn_reward(self, state, action, reward):\n",
    "      self.reward_model[state, action] = reward\n",
    "\n",
    "   # This function draws all transitions from the real maze and checks whether the model outputs the same reward.\n",
    "   # This way to proceed is only adequate if the real maze is deterministic.\n",
    "\n",
    "   def is_accurate(self, env) -> bool:\n",
    "      # To be completed...\n",
    "\n",
    "      assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "   # Useful for debug\n",
    "   \n",
    "   def display_reward(self):\n",
    "      print(\"reward model\", self.reward_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93276201",
   "metadata": {},
   "source": [
    "## Termination model\n",
    "\n",
    "This class is used to learn a model of the termination function.\n",
    "We consider that termination is deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f77feb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TerminationModel():\n",
    "   def __init__(self, mdp):\n",
    "      self.nb_states = mdp.unwrapped.nb_states\n",
    "      self.nb_actions = mdp.action_space.n\n",
    "      self.termination_model = np.zeros((self.nb_states, self.nb_actions))\n",
    "\n",
    "   # This function draws all transitions from the real maze and checks whether the model outputs the same termination\n",
    "   # This way to proceed is only adequate if the real maze is deterministic.\n",
    "\n",
    "   def is_accurate(self, env) -> bool:\n",
    "      for state in range(self.nb_states):\n",
    "         for action in range(self.nb_actions):\n",
    "            t_model = self.termination_model[state, action]\n",
    "            env.mdp.current_state = state # ugly need to set the state from outside\n",
    "            _, _, terminated, _, _ = env.step(action)\n",
    "            t_ground_truth = terminated\n",
    "            if not (t_model == t_ground_truth):\n",
    "               # print(state, action, t_model, t_ground_truth)\n",
    "               return False\n",
    "      return True\n",
    "   \n",
    "   def predict(self, state, action):\n",
    "      return self.termination_model[state, action]\n",
    "\n",
    "   def learn_termination(self, state, action, termination):\n",
    "      self.termination_model[state, action] = termination\n",
    "\n",
    "   # Useful for debug\n",
    "\n",
    "   def display_termination(self):\n",
    "      print(\"terminated model\", self.termination_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53325b",
   "metadata": {},
   "source": [
    "## Learn a forward model with a random policy ##\n",
    "\n",
    "In the function below, we train a transition model, a reward model and a termination model from\n",
    "an agent performing random actions\n",
    "We stop once the transition model and the reward model are accurate enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a6dcf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def learn_forward_model_from_random_actions(\n",
    "    mdp: MazeMDPEnv,\n",
    "    timeout: int = 50,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "\n",
    "   trans_model = StochasticTransitionModel(env)\n",
    "   reward_model = RewardModel(env)\n",
    "   terminated_model = TerminationModel(env)\n",
    "\n",
    "   # Run learning cycle\n",
    "   mdp.timeout = timeout # episode length\n",
    "   steps = 0 # number of steps before convergence\n",
    "\n",
    "   # To be completed...\n",
    "\n",
    "   assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "   print(f\"the number of steps needed to learn accurate models from random actions was {steps}\")\n",
    "   return trans_model, reward_model, terminated_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc52949",
   "metadata": {},
   "source": [
    "## Q-learning agents ##\n",
    "\n",
    "We will reuse the Q-learning algorithm coded in the previous lab.\n",
    "But this time, we write it into a more object oriented way.\n",
    "\n",
    "We start with a general QAgent function and then derive it into SoftmaxQAgent and EgreedyQAgent to account for two exploration methods\n",
    "\n",
    "In the code below,\n",
    "- fill the updateQ function\n",
    "\n",
    "- write the update_Q_from_models function that updates the Q-table from models of the transition, reward and termination functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd92c0e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "   def __init__(self, mdp, alpha):\n",
    "      self.nb_states = mdp.unwrapped.nb_states\n",
    "      self.nb_actions = mdp.action_space.n\n",
    "      self.gamma = mdp.unwrapped.gamma\n",
    "      self.alpha = alpha\n",
    "      self.Q = np.zeros((self.nb_states, self.nb_actions))\n",
    "\n",
    "   def choose_action(self, state):\n",
    "      pass\n",
    "\n",
    "   # Beware that the efficiency is highly dependent on the error threshold\n",
    "   def is_accurate(self, q_table) -> bool:\n",
    "      error = np.linalg.norm(self.Q - q_table)\n",
    "      if error > 0.01:\n",
    "         # print(\"self :\", self.Q)\n",
    "         # print(\"table :\", q_table)\n",
    "         # print(f\"QAgent error: {error}\")\n",
    "         return False\n",
    "      return True\n",
    "\n",
    "   # Do not forget to deal with the case where the episode is terminated\n",
    "   def updateQ(self, state, action, reward, next_state, terminated) -> None:\n",
    "      \"\"\"\n",
    "      Performs a Bellman back-up over the Q-table of the agent\n",
    "      :return: nothing\n",
    "      \"\"\"\n",
    "      # To be completed...\n",
    "\n",
    "      assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "   # The function below should use the above function\n",
    "   def update_Q_from_models(self, trans_model:TransitionModel, reward_model:RewardModel, terminated_model:TerminationModel, nb_updates: int) -> None:\n",
    "      \"\"\"\n",
    "      Updates the Q-table of the agent using randomly sampled transitions (i.e. the agent is learning in imagination)\n",
    "      It does so nb_updates times\n",
    "      :param trans_model: the model of the transition function used to sample updates\n",
    "      :param reward_model: the model of the reward function used to compute the reward of the sample updates\n",
    "      :param terminated_model: the model of the termination function used to determine if the sample updates are terminal\n",
    "      :param nb_updates: the number of performed updates\n",
    "      :return: nothing\n",
    "      \"\"\"\n",
    "      # To be completed...\n",
    "\n",
    "      assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52a07c",
   "metadata": {},
   "source": [
    "Fill the choose_action function of both classes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737cd9d2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SoftmaxQAgent(QAgent):\n",
    "   def __init__(self, mdp, alpha: float = 0.5, beta: float = 6.0):\n",
    "      super().__init__(mdp, alpha)\n",
    "      self.beta = beta\n",
    "\n",
    "   def choose_action(self, state) -> int:\n",
    "      # To be completed...\n",
    "\n",
    "      assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36014b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EgreedyQAgent(QAgent):\n",
    "   def __init__(self, mdp, alpha: float = 0.5, epsilon: float = 0.02):\n",
    "      super().__init__(mdp, alpha)\n",
    "      self.epsilon = epsilon \n",
    "\n",
    "   def choose_action_egreedy(self, state) -> int:\n",
    "      # To be completed...\n",
    "\n",
    "      assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0127238",
   "metadata": {},
   "source": [
    "## Import the Value iteration with the $Q$ function algorithm from the dynamic programming lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d2c0c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9859f3",
   "metadata": {},
   "source": [
    "## Learn the Q-table using the learned models\n",
    "\n",
    "In the function below, we train a QAgent from samples taken randomly from a transition model, a reward model and a termination model\n",
    "We stop once the QAgent is accurate enough, compared to a Q-table obtained from value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e37620",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def learn_Q_from_models(\n",
    "   agent: QAgent,\n",
    "   trans_model: StochasticTransitionModel,\n",
    "   reward_model: RewardModel,\n",
    "   terminated_model: TerminationModel,\n",
    "   q_table,\n",
    "):\n",
    "   steps = 0 # number of steps before convergence\n",
    "\n",
    "   # To be completed...\n",
    "\n",
    "   assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "   return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9163b0a1",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Run the model learning algorithm then get an optimal Q-table from that model\n",
    "We do it several times to obtain an average number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6fa4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_repeats = 10\n",
    "TIMEOUT = 200\n",
    "\n",
    "steps = np.zeros(nb_repeats)\n",
    "for i in range(nb_repeats):\n",
    "   trans_model, reward_model, terminated_model = learn_forward_model_from_random_actions(env, timeout=TIMEOUT)\n",
    "   agent = SoftmaxQAgent(env)\n",
    "   q_table, _ = value_iteration_q(env, render=False)\n",
    "   steps[i] = learn_Q_from_models(agent, trans_model, reward_model, terminated_model, q_table)\n",
    "   print(f\"the number of steps needed to learn a perfect Q-Table from the models was {steps[i]}\")\n",
    "print(f\"the mean number of steps needed to learn a perfect Q-Table from the models was {steps.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c88a0",
   "metadata": {},
   "source": [
    "## Dyna-Q algorithm\n",
    "\n",
    "This time, we learn the models and the Q functions simultaneously\n",
    "We perform `nb_updates` steps \"in imagination\" for each step in the real environment\n",
    "We stop once the QAgent is accurate enough, compared to a Q-table obtained from value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65317c7d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def dyna_q_soft(\n",
    "    mdp: MazeMDPEnv,\n",
    "    q_table,\n",
    "    nb_updates: int = 5,\n",
    "    timeout: int = 50,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "   # Initialize the state-action value function\n",
    "   # alpha is the learning rate\n",
    "   agent = SoftmaxQAgent(mdp)\n",
    "   trans_model = StochasticTransitionModel(env)\n",
    "   reward_model = RewardModel(env)\n",
    "   terminated_model = TerminationModel(env)\n",
    "\n",
    "   # Run learning cycle\n",
    "   mdp.timeout = timeout  # episode length\n",
    "   steps = 0\n",
    "\n",
    "   # To be completed...\n",
    "\n",
    "   assert False, 'Not implemented yet'\n",
    "\n",
    "   return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e9649",
   "metadata": {},
   "source": [
    "Run the Dyna-Q algorithm for different values of nb_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEOUT = 200\n",
    "q_table, _ = value_iteration_q(env, render=False)\n",
    "for nb_updates in range(10):\n",
    "   step_array = np.zeros(10)\n",
    "   for i in range(10):\n",
    "      step_array[i] = dyna_q_soft(env, q_table, nb_updates, timeout=TIMEOUT)\n",
    "   print(f\"the number of steps needed to learn a perfect Q-Table with Dyna-Q using {nb_updates} updates was {step_array.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2eb216",
   "metadata": {},
   "source": [
    "## Empirical study\n",
    "\n",
    "- Compare the number of samples used to first learn the model using random actions, then deriving an optimal Q-table from the model, versus improving the Q-table and the behavior of the agent simultaneously.\n",
    "- Do it for various numbers of updates \"in imagination\" in the simultaneous learning case.\n",
    "- Try it using the Deterministic model and the Stochastic model, and using Softmax Exploration versus Egreedy exploration.\n",
    "- Finally, conclude"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
