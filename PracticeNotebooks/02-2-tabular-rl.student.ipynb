{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bba7a75",
   "metadata": {},
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the\n",
    " LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9cb600",
   "metadata": {},
   "source": [
    "Outlook\n",
    "In this notebook we will study basic reinforcement learning algorithms: TD learning, q-learning and sarsa. We will also investigate two basic exploration strategies: $\\epsilon$-greedy and softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install 'easypip>=1.2.0'\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "easyimport(\"bbrl_gymnasium\")\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tqdm was removed from q-learning and sarsa because when we use them\n",
    "# many times in a loop (e.g. for hyper-param tuning), it slows down the computation a lot\n",
    "if is_notebook() and get_ipython().__class__.__module__ != \"google.colab._shell\":\n",
    "   from tqdm.autonotebook import tqdm\n",
    "else:\n",
    "   from tqdm.auto import tqdm\n",
    "\n",
    "from tabularmazemdp.toolbox import egreedy, egreedy_loc, softmax, sample_categorical\n",
    "from tabularmazemdp.mdp import Mdp\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "# For visualization\n",
    "os.environ[\"VIDEO_FPS\"] = \"5\"\n",
    "if not os.path.isdir(\"./videos\"):\n",
    "    os.mkdir(\"./videos\")\n",
    "\n",
    "from IPython.display import Video\n",
    "\n",
    "# Settings\n",
    "NB_EPISODES = 100\n",
    "TIMEOUT = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f1202",
   "metadata": {},
   "source": [
    "Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning is about finding the optimal policy in an MDP which is initially unknown to the agent.\n",
    "More precisely, the state and action spaces are known, but the agent does not know the transition and reward functions.\n",
    "Generally speaking, the agent has to explore the MDP to figure out which action in which state leads to which other state and reward.\n",
    "The model-free case is about finding this optimal policy just through very local updates, without storing any information about previous interactions with the environment.\n",
    "Principles of these local updates can already be found in the Temporal Difference (TD) algorithm, which iteratively computes optimal values for all state using local updates.\n",
    "The most widely used model-free RL algorithms are **q-learning**, **SARSA** and **actor-critic** algorithms.\n",
    "\n",
    "As for dynamic programming, we first create a maze-like MDP. Reinforcement learning is slower than dynamic programming, so we will work with smaller mazes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5343db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import bbrl_gymnasium\n",
    "\n",
    "# Environment with 20% of walls and no negative reward when hitting a wall\n",
    "env = gym.make(\"MazeMDP-v0\", kwargs={\"width\": 4, \"height\": 3, \"ratio\": 0.2, \"hit\": 0.0}, render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "env.init_draw(\"The maze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aed3e8",
   "metadata": {},
   "source": [
    "# Temporal Difference (TD) learning ##\n",
    "\n",
    "Given a state and an action spaces as well as a policy, TD(0) computes the state value of this policy based on the following equations:\n",
    "$$\\delta_t = r(s_t,a_t) + \\gamma V^{(t)}(s_{t+1})-V^{(t)}(s_t)$$ \n",
    "$$V^{(t+1)}(s_t) = V^{(t)}(s_t) + \\alpha\\delta_t$$\n",
    "\n",
    "where $\\delta$ is the TD error and $\\alpha$ is a parameter called \"learning rate\".\n",
    "\n",
    "The code is provided below, so that you can take inspiration later on. The important part is the computation of $\\delta$, and the update of the values of $V$.\n",
    "\n",
    "To run TD learning, a policy is needed as input. Such a policy can be retreived by using the `policy_iteration_q(mdp)` function defined in the dynamic programming notebook.\n",
    "\n",
    "If you want to run this notebook independently, you can use instead the `random_policy` provided in `mazemdp`.\n",
    "This is what we do here by default, replace it if you want to run TD learning from an optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a68314",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from tabularmazemdp import random_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92eda4",
   "metadata": {},
   "source": [
    "**Question:** In the code of the *temporal_difference(...)* function below, fill the missing parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a503b31",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def temporal_difference(\n",
    "    mdp: MazeMDPEnv,\n",
    "    policy: np.ndarray,\n",
    "    nb_episodes: int = 50,\n",
    "    alpha: float = 0.2,\n",
    "    timeout: int = 25,\n",
    "    render: bool = True,\n",
    ") -> np.ndarray:\n",
    "    # alpha: learning rate\n",
    "    # timeout: timeout of an episode (maximum number of timesteps)\n",
    "    v = np.zeros(mdp.nb_states)  # initial state value v\n",
    "    mdp.timeout = timeout\n",
    "\n",
    "    if render:\n",
    "       video_recorder = VideoRecorder(mdp, \"videos/TemporalDifferences.mp4\", enabled=render)\n",
    "       mdp.init_draw(\"Temporal differences\", recorder=video_recorder)\n",
    "\n",
    "    for _ in tqdm(range(nb_episodes)):  # for each episode\n",
    "\n",
    "        # Draw an initial state randomly (if uniform is set to False, the state\n",
    "        # is drawn according to the P0 distribution)\n",
    "        x, _ = mdp.reset(uniform=True)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        while not (terminated or truncated):\n",
    "           # Show agent\n",
    "           if render:\n",
    "              mdp.draw_v_pi(v, policy, recorder=video_recorder)\n",
    "\n",
    "           # Step forward following the MDP: x=current state, pol[i]=agent's\n",
    "           # action according to policy pol, r=reward gained after taking\n",
    "           # action pol[i], terminated=tells whether  the episode ended, and info\n",
    "           # gives some info about the process\n",
    "           y, r, terminated, truncated, _ = mdp.step(egreedy_loc(policy[x], mdp.action_space.n, epsilon=0.2))\n",
    "           # To be completed...\n",
    "\n",
    "           # Update the state value of x\n",
    "           delta = ...\n",
    "           v[x] = ...\n",
    "           assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "           # Update agent's position (state)\n",
    "           x = y\n",
    "\n",
    "    if render:\n",
    "       mdp.current_state = 0\n",
    "       mdp.draw_v_pi(v, policy, recorder=video_recorder)\n",
    "       video_recorder.close()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a22f33",
   "metadata": {},
   "source": [
    "Once this is done, you can run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c06d9f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "policy = random_policy(env)\n",
    "v = temporal_difference(env, policy, nb_episodes=10, timeout=TIMEOUT)\n",
    "\n",
    "video_display(\"videos/TemporalDifferences.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88bcacc",
   "metadata": {},
   "source": [
    "Unless you were lucky, the generated value function is boring: if the policy does not reach the final state, all values are 0. To avoid this, you can copy-paste a dynamic programming function on the Q function from the previous notebook, use it to get an optimal policy, and use this policy for TD learning. You should get a much more interesting value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1072a8d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Put your code to obtain an optimal Q function here\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d0ea0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Put your code to get a policy from a Q function here\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code to run the algorithm here\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd39e53",
   "metadata": {},
   "source": [
    "# Q-learning ##\n",
    "\n",
    "The **q-learning** algorithm accounts for an agent exploring an MDP and\n",
    "updating at each step a model of the state action-value function stored into a\n",
    "Q-table. It is updated as follows:\n",
    "\n",
    "$$\n",
    "\\delta_t = r(s_t,a_t) + \\gamma \\max_{a \\in A}\n",
    "Q^{(t)}(s_{t+1},a)-Q^{(t)}(s_t,a_t)\n",
    "$$\n",
    "\n",
    "$$Q^{(t+1)}(s_t,a_t) = Q^{(t)}(s_t,a_t) + \\alpha \\delta_t$$\n",
    "\n",
    "To visualize the policy, we need the `get_policy_from_q(q)` function that we defined in the\n",
    "dynamic programming notebook. If you have not done so yet, import it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe72ee6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b996ac",
   "metadata": {},
   "source": [
    "Fill the code of the `q_learning(...)` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba16ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --------------------------- Q-Learning epsilon-greedy version -------------------------------#\n",
    "# Given an exploration rate epsilon, the QLearning algorithm computes the state action-value function\n",
    "# based on an epsilon-greedy policy\n",
    "# alpha is the learning rate\n",
    "\n",
    "def q_learning_eps(\n",
    "    mdp: MazeMDPEnv,\n",
    "    epsilon: float,\n",
    "    nb_episodes: int = 20,\n",
    "    timeout: int = 50,\n",
    "    alpha: float = 0.5,\n",
    "    render: bool = True,\n",
    "    init_q: float = 0.,\n",
    "    uniform: bool =  True\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    # Initialize the state-action value function\n",
    "    # alpha is the learning rate\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q[:, :] = init_q\n",
    "    q_list = []\n",
    "    time_list = []\n",
    "\n",
    "    # Run learning cycle\n",
    "    mdp.timeout = timeout  # episode length\n",
    "\n",
    "    if render:\n",
    "       video_recorder = VideoRecorder(mdp, \"videos/Q-Learning.mp4\", enabled=render)\n",
    "       mdp.init_draw(\"Q Learning\", recorder=video_recorder)\n",
    "    \n",
    "    for _ in range(nb_episodes):\n",
    "        # Draw the first state of episode i using a uniform distribution over all the states\n",
    "        x, _ = mdp.reset(uniform=uniform)\n",
    "        cpt = 0\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        while not (terminated or truncated):\n",
    "            # Show the agent in the maze\n",
    "            if render:\n",
    "              mdp.draw_v_pi(q, q.argmax(axis=1), recorder=video_recorder)\n",
    "\n",
    "            # Draw an action using an epsilon-greedy policy\n",
    "            u = egreedy(q, x, epsilon)\n",
    "\n",
    "            # Perform a step of the MDP\n",
    "            y, r, terminated, truncated, _ = mdp.step(u)\n",
    "\n",
    "            # To be completed...\n",
    "\n",
    "            # Update the state-action value function with q-Learning\n",
    "            delta = ...\n",
    "            q[...] = ...\n",
    "            assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "            # Update the agent position\n",
    "            x = y\n",
    "            cpt = cpt + 1\n",
    "            \n",
    "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n",
    "        time_list.append(cpt)\n",
    "\n",
    "    if render:\n",
    "       mdp.current_state = 0\n",
    "       mdp.draw_v_pi(q, get_policy_from_q(q), recorder=video_recorder)\n",
    "       video_recorder.close()\n",
    "\n",
    "    return q, q_list, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d826299",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "And run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9861ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.02\n",
    "q, q_list, time_list = q_learning_eps(env, epsilon, nb_episodes=NB_EPISODES, timeout=TIMEOUT)\n",
    "video_display(\"videos/Q-Learning.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a7cff",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Harder case: fixed starting point and exploration\n",
    "\n",
    "We now explore the case where the agent always start at the *beginning of the maze* (`uniform=False`), corresponding to the top-left corner when this is a free cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fde2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.02\n",
    "start_q, start_q_list, time_list = q_learning_eps(env, epsilon, nb_episodes=NB_EPISODES, timeout=TIMEOUT, uniform=False)\n",
    "video_display(\"videos/Q-Learning.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aabe48",
   "metadata": {},
   "source": [
    "You will observe that it is very difficult for the agent to learn to reach the\n",
    "final state (and the larger the maze, the more difficult). A simple trick to\n",
    "avoid this is to initialize the value of each $(s,a)$ pair to a small (lower\n",
    "than the final reward) value. Try it with the example above !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f19bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "# Put your code to run q_learning_eps here\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696ef42",
   "metadata": {},
   "source": [
    "### Learning dynamics\n",
    "\n",
    "By watching carefully the values while the agent is learning, you can see that\n",
    "the agent favors certains paths over others which have a strictly equivalent value.\n",
    "This can be explained easily: as the agent chooses a path for the first\n",
    "time, it updates the values along that path, these values get higher than the\n",
    "surrounding values, and the agent chooses the same path again and again,\n",
    "increasing the phenomenon. Only steps of random exploration can counterbalance\n",
    "this effect, but they do so extremely slowly.\n",
    "\n",
    "### Exploration\n",
    "\n",
    "In the `q_learning(...)` function above, action selection is based on a\n",
    "$\\epsilon$-greedy policy. Instead, it could have relied on *`softmax`*.\n",
    "\n",
    "In the function below, you have to replace the call to the\n",
    "previous *$\\epsilon$-greedy* policy with a `softmax` policy. The\n",
    "`softmax(...)` and `egreedy(...)` functions are available in\n",
    "`tabularmazemdp.toolbox`. \n",
    "\n",
    "`sofmax(...)` returns a distribution probability over actions. To sample\n",
    "an action according to their probabilities, you can use the\n",
    "`sample_categorical` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Q-Learning softmax version ----------------------------#\n",
    "# Given a temperature \"beta\", the QLearning algorithm computes the state action-value function\n",
    "# based on a softmax policy\n",
    "# alpha is the learning rate\n",
    "\n",
    "def q_learning_soft(\n",
    "    mdp: MazeMDPEnv,\n",
    "    beta: float,\n",
    "    nb_episodes: int = 20,\n",
    "    timeout: int = 50,\n",
    "    alpha: float = 0.5,\n",
    "    render: bool = True,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    # Initialize the state-action value function\n",
    "    # alpha is the learning rate\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_list = []\n",
    "    time_list = []\n",
    "\n",
    "    # Run learning cycle\n",
    "    mdp.timeout = timeout  # episode length\n",
    "\n",
    "    if render:\n",
    "       video_recorder = VideoRecorder(mdp, \"videos/QLearningSoftmax.mp4\", enabled=render)\n",
    "       mdp.init_draw( \"Q Learning (Softmax)\", recorder=video_recorder)\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        # Draw the first state of episode i using a uniform distribution over all the states\n",
    "        x, _ = mdp.reset(uniform=True)\n",
    "        cpt = 0\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        while not (terminated or truncated):\n",
    "            if render:\n",
    "              mdp.draw_v_pi(q, q.argmax(axis=1), recorder=video_recorder)\n",
    "            \n",
    "            # To be completed...\n",
    "\n",
    "            # Draw an action using a soft-max policy\n",
    "            u = ... # (here, call the softmax function)\n",
    "            assert False, 'Not implemented yet'\n",
    "\n",
    "            \n",
    "            # To be completed...\n",
    "\n",
    "            # Copy-paste the rest from q_learning_eps\n",
    "            assert False, 'Not implemented yet'\n",
    "\n",
    "            \n",
    "            x = y\n",
    "            cpt = cpt + 1\n",
    "            \n",
    "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n",
    "        time_list.append(cpt)\n",
    "        \n",
    "    if render:\n",
    "       mdp.current_state = 0\n",
    "       mdp.draw_v_pi(q, get_policy_from_q(q), recorder=video_recorder)\n",
    "       video_recorder.close()\n",
    "    \n",
    "    return q, q_list, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b22446",
   "metadata": {},
   "source": [
    " Run this new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPISODES = 40\n",
    "beta = 6\n",
    "q, q_list, time_list = q_learning_soft(env, beta, nb_episodes=NB_EPISODES, timeout=TIMEOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c227c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "video_display(\"videos/QLearningSoftmax.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe0f4b",
   "metadata": {},
   "source": [
    "# Sarsa\n",
    "\n",
    "The **sarsa** algorithm is very similar to **q-learning**. At first glance,\n",
    "the only difference is in the update rule. However, to perform the update in\n",
    "**sarsa**, one needs to know the action the agent will take when it will be at\n",
    "the next state, even if the agent is taking a random action.\n",
    "\n",
    "This implies that the next state action is determined in advance and stored\n",
    "for being played at the next time step.\n",
    "\n",
    "Fill the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce96c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --------------------------- Sarsa, epsilon-greedy version -------------------------------#\n",
    "# Given an exploration rate epsilon, the SARSA algorithm computes the state action-value function\n",
    "# based on an epsilon-greedy policy\n",
    "# alpha is the learning rate\n",
    "\n",
    "def sarsa_eps(\n",
    "    mdp: MazeMDPEnv,\n",
    "    epsilon: float,\n",
    "    nb_episodes: int = 20,\n",
    "    timeout: int = 50,\n",
    "    alpha: float = 0.5,\n",
    "    render: bool = True,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    # Initialize the state-action value function\n",
    "    # alpha is the learning rate\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_list = []\n",
    "    time_list = []\n",
    "\n",
    "    # Run learning cycle\n",
    "    mdp.timeout = timeout  # episode length\n",
    "\n",
    "    if render:\n",
    "       video_recorder = VideoRecorder(mdp, \"videos/Sarsae.mp4\", enabled=render)\n",
    "       mdp.init_draw( \"Sarsa e-greedy\", recorder=video_recorder)\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        # Draw the first state of episode i using a uniform distribution over all the states\n",
    "        x, _ = mdp.reset(uniform=True)\n",
    "        cpt = 0\n",
    "\n",
    "        # To be completed...\n",
    "\n",
    "        # Fill this part of the code\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n",
    "        time_list.append(cpt)\n",
    "\n",
    "    if render:\n",
    "       mdp.current_state = 0\n",
    "       mdp.draw_v_pi(q, get_policy_from_q(q), recorder=video_recorder)\n",
    "       video_recorder.close()\n",
    "    return q, q_list, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b40945",
   "metadata": {},
   "source": [
    "And run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.02\n",
    "q, q_list, time_list = sarsa_eps(env, epsilon, nb_episodes=NB_EPISODES, timeout=TIMEOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2901f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_display(\"videos/Sarsae.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d08c4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "As for **q-learning** above, copy-paste the resulting code to get a\n",
    "*sarsa_soft(...)* and a *sarsa_eps(...)* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Sarsa, softmax version -------------------------------#\n",
    "# Given a temperature \"beta\", the SARSA algorithm computes the state action-value function\n",
    "# based on a softmax policy\n",
    "# alpha is the learning rate\n",
    "\n",
    "def sarsa_soft(\n",
    "    mdp: MazeMDPEnv,\n",
    "    beta: float,\n",
    "    nb_episodes: int = 20,\n",
    "    timeout: int = 50,\n",
    "    alpha: float = 0.5,\n",
    "    render: bool = True,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "\n",
    "    # Initialize the state-action value function\n",
    "    # alpha is the learning rate\n",
    "    q = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_min = np.zeros((mdp.nb_states, mdp.action_space.n))\n",
    "    q_list = []\n",
    "    time_list = []\n",
    "\n",
    "    # Run learning cycle\n",
    "    mdp.timeout = timeout  # episode length\n",
    "\n",
    "    if render:\n",
    "       video_recorder = VideoRecorder(mdp, \"videos/Sarsa-softmax.mp4\", enabled=render)\n",
    "       mdp.init_draw( \"Sarsa (Softmax)\", recorder=video_recorder)\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        # Draw the first state of episode i using a uniform distribution over all the states\n",
    "        x, _ = mdp.reset(uniform=True)\n",
    "        cpt = 0\n",
    "\n",
    "        # To be completed...\n",
    "\n",
    "        # Fill this part of the code\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        q_list.append(np.linalg.norm(np.maximum(q, q_min)))\n",
    "        time_list.append(cpt)\n",
    "\n",
    "    if render:\n",
    "       mdp.current_state = 0\n",
    "       mdp.draw_v_pi(q, get_policy_from_q(q), recorder=video_recorder)\n",
    "       video_recorder.close()\n",
    "    return q, q_list, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d20382",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "And run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98430b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "# Put your code to run sarsa_soft here\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f077fe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "video_display(\"videos/Sarsa-softmax.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aec7e6",
   "metadata": {},
   "source": [
    "## Study part\n",
    "### Impact of `epsilon` and `beta` on q-learning and sarsa\n",
    "\n",
    "Compare the number of steps needed by **q-learning** and **sarsa** to converge\n",
    "on a given MDP using the *softmax* and *$\\epsilon$-greedy* exploration\n",
    "strategies. To figure out, you can use the provided `plot_ql_sarsa(m, epsilon,\n",
    "beta, nb_episodes, timeout, alpha, render)` function below with various values\n",
    "for $\\epsilon$ (e.g. 0.001, 0.01, 0.1) and $\\beta$ (e.g. 0.1, 5, 10) and\n",
    "comment the obtained curves. Other visualizations are welcome.\n",
    "\n",
    "Note that instead of the temperature `beta`, computational neuroscience\n",
    "researchers, who generally prefer softmax exploration, use a parameter `beta`\n",
    "which behaves as an inverse of the temperature. That way, the three\n",
    "hyper-parameters of basic tabular RL algorithms are `alpha`, `beta`, and\n",
    "`gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b29b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -------- plot learning curves of Q-Learning and Sarsa using epsilon-greedy and softmax ----------#\n",
    "def plot_ql_sarsa(env, epsilon, beta, nb_episodes, timeout, alpha, render):\n",
    "    q, q_list1, time_list1 = q_learning_eps(env, epsilon, nb_episodes, timeout, alpha, render)\n",
    "    q, q_list2, time_list2 = q_learning_soft(env, beta, nb_episodes, timeout, alpha, render)\n",
    "    q, q_list3, time_list3 = sarsa_eps(env, epsilon, nb_episodes, timeout, alpha, render)\n",
    "    q, q_list4, time_list4 = sarsa_soft(env, beta, nb_episodes, timeout, alpha, render)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(range(len(q_list1)), q_list1, label='q-learning e-greedy')\n",
    "    plt.plot(range(len(q_list2)), q_list2, label='q-learning softmax')\n",
    "    plt.plot(range(len(q_list3)), q_list3, label='sarsa e-greedy')\n",
    "    plt.plot(range(len(q_list4)), q_list4, label='sarsa softmax')\n",
    "\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('Norm of Q values')\n",
    "    plt.legend(loc='upper right')\n",
    "    # plt.savefig(\"comparison_RL.png\")\n",
    "    plt.title(\"Comparison of convergence rates\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(time_list1)), time_list1, label='qlearning e-greedy')\n",
    "    plt.plot(range(len(time_list2)), time_list2, label='qlearning softmax')\n",
    "    plt.plot(range(len(time_list3)), time_list3, label='sarsa e-greedy')\n",
    "    plt.plot(range(len(time_list4)), time_list4, label='sarsa softmax')\n",
    "\n",
    "    plt.xlabel('Number of episodes')\n",
    "    plt.ylabel('Steps to reach goal')\n",
    "    plt.legend(loc='upper right')\n",
    "    # plt.savefig(\"comparison_RL.png\")\n",
    "    plt.title(\"test\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5caf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "plot_ql_sarsa(env, epsilon=0.02, beta=6, nb_episodes=1000, timeout=100, alpha=0.5, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f481bc3",
   "metadata": {},
   "source": [
    "### Effect of hyper-parameters\n",
    "\n",
    "The other two hyper-parameters of **q-learning** and **sarsa** are $\\alpha$,\n",
    "and $\\gamma$. By varying the values of these hyper-parameters and watching the\n",
    "learning process and behavior of the agent, explain their impact on the\n",
    "algorithm. Using additional plotting functions is also welcome.\n",
    "\n",
    "A good idea to visualize the effect of two parameters is to generate a heat map\n",
    "by letting both parameters take values in a well-chosen interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51afdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "# Put your visualization code here\n",
    "assert False, 'Not implemented yet'\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
